# -*- coding: utf-8 -*-
"""train_custom.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Su7l0em_b8su8qMxDQAhAbWxmUtjFjNY
"""

import matplotlib.pyplot as plt
import numpy as np
import os
import tensorflow as tf
from tensorflow import keras
import time
import pathlib
from tensorflow.keras import models, layers, datasets, optimizers
from tensorflow.keras import  Input
from tensorflow.keras.layers import Conv2D, BatchNormalization, Add, Activation, MaxPooling2D, Dropout, Flatten, Dense
from datetime import datetime

from google.colab import drive
drive.mount('/content/drive')

data_dir = "/content/drive/My Drive/vision/dataset"
os.chdir(data_dir)
#data_dir = pathlib.Path(data_dir)

# medium-high size
# correlated to momentum of momentum and lr
batch_size = 64

AUTOTUNE = tf.data.experimental.AUTOTUNE

# fixed squared images
img_height = 256
img_width = 256
print(data_dir)

# Defaults image size to (256, 256)
train_ds = tf.keras.preprocessing.image_dataset_from_directory(
  data_dir,
  validation_split=0.2,
  subset="training",
  seed=123, # rnd generation default
  image_size=(img_height, img_width),
  batch_size=batch_size
)

val_ds = tf.keras.preprocessing.image_dataset_from_directory(
  data_dir,
  validation_split=0.2,
  subset="validation",
  seed=123, # rnd generation default
  image_size=(img_height, img_width),
  batch_size=batch_size
)

class_names = ['covid','non-covid']
print(class_names)

train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)
val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)

###############################
#      DATA AUGMENTATION      #
###############################
# mirroring
# rotation
# contrast jitter
# center crop

data_augmentation = keras.Sequential(
  [
    # mirroring
    layers.experimental.preprocessing.RandomFlip("horizontal", input_shape=(img_height, img_width,3)),
    layers.experimental.preprocessing.CenterCrop(128,128),
    layers.experimental.preprocessing.RandomContrast(samples, height, width, channels)
    layers.experimental.preprocessing.RandomRotation(0.1),
    # layers.experimental.preprocessing.RandomTranslation(0.1,0.1),
    # layers.experimental.preprocessing.RandomZoom(0.2),
    # layers.GaussianNoise(stddev=1)   # standard deviation of noise distribution
  ]
)

###############################
#          CNN MODEL          #
###############################

model = tf.keras.Sequential([
  data_augmentation,
  layers.experimental.preprocessing.Rescaling(1./255), # scaling image from [0,255] -> [0,1]

  layers.Conv2D(16, 5, padding='valid', activation='relu'),  
  layers.MaxPooling2D((2,2)), # stride default
  layers.BatchNormalization(momentum=0.8),
  
  layers.Conv2D(32, 3, padding='same', activation='relu'),
  layers.MaxPooling2D((2,2)),
  layers.BatchNormalization(momentum=0.8),

  layers.Conv2D(64, 3, padding='same', activation='relu'),
  layers.MaxPooling2D((2,2)),
  layers.BatchNormalization(momentum=0.8),

  layers.Conv2D(128, 3, padding='same', activation='relu'),
  layers.MaxPooling2D((2, 2)),
  layers.BatchNormalization(momentum=0.8),

  layers.Dropout(0.5),
  layers.Flatten(),

  #change activation
  layers.Dense(256, activation='relu'),  # first fully connected layers
  layers.Dense(2) # 2 is actually our number of classes. #last fully con layer

])

# LR DECAY with STOCASTIC GRAD DESCENT
lr_schedule = keras.optimizers.schedules.ExponentialDecay(
    initial_learning_rate=1e-2,
    decay_steps=10000,
    decay_rate=0.9)
opt = keras.optimizers.Adam(learning_rate=lr_schedule)

model.compile(
  optimizer=opt,
  loss= tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
  # loss= tf.keras.losses.BinaryCrossEntropy(from_logits=True) # bad
  metrics=['accuracy'])

print(model.summary())

# n of epochs
epochs = 80
# epochs = 50

history = model.fit(
  train_ds,
  validation_data=val_ds,
  epochs=epochs,
  batch_size=batch_size,
  shuffle = 1040 # greater or equal dataset size
)

# save model name
model.save('model.h5')

# stats
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss=history.history['loss']
val_loss=history.history['val_loss']

epochs_range = range(epochs)

plt.figure(figsize=(8, 8))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')


plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
# plt.savefig()