# -*- coding: utf-8 -*-
"""Inference.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yI-2Y4FNIA1gbVo0KHEvOjHatpc8xVle

Import libraries
"""

import matplotlib.pyplot as plt
import numpy as np
import os
import tensorflow as tf
from tensorflow import keras
import time
import pathlib
from tensorflow.keras import models, layers, datasets, optimizers
from tensorflow.keras import  Input
from tensorflow.keras.layers import Conv2D, BatchNormalization, Add, Activation, MaxPooling2D, Dropout, Flatten, Dense
from datetime import datetime
now = datetime.now() 
from sklearn.metrics import confusion_matrix
import seaborn as sn
import pandas as pd

# MOUNT TO DRIVE
from google.colab import drive
drive.mount('/content/drive')

# THIS FUNCTION TESTS THE GIVEN MODEL OVER THE GIVEN DATASET PRODUCING PERFORMANCES EVALUATION

def inference (path_to_model, path_to_dataset_covid, path_to_dataset_nocovid):

  # MODEL
  model = keras.models.load_model(path_to_model)

  # POSITIVES 
  data_dir_covid = pathlib.Path(path_to_dataset_covid)
  # NEGATIVES
  data_dir_nocovid = pathlib.Path(path_to_dataset_nocovid)
               
  # create a list of elems for positives
  positives = list(data_dir_covid.glob('*'))
  # create a list of negative elems
  negatives = list(data_dir_nocovid.glob('*'))
  print("\nTotal n. of samples for \nPositives : ",len(positives),"\nNegatives : ",len(negatives),"\n")

  # initializing indices
  true_positive = 0
  false_positive = 0 
  true_negative = 0
  false_negative = 0

  # ----------------------------------------------------------------------------
  ## Run Over all positives
  #-----------------------------------------------------------------------------

  print("\nAnalyzing positives...\n")

  predictions_list_positives = []
  ground_truth_list_positives = np.zeros(len(positives))

  for sample in positives:
    # read the image
    img = keras.preprocessing.image.load_img(str(sample), target_size=(256, 256))
    # trasform into array
    img_array = keras.preprocessing.image.img_to_array(img)
    img_array = tf.expand_dims(img_array, 0)
    
    # apply your inference here
    predictions = model.predict(img_array)
    score = tf.nn.softmax(predictions[0])

    #class_predict = tf.nn.sigmoid_cross_entropy_with_logits(labels=1, logits=np.max(predictions[0]))
    #print(np.max(class_predict))
    
    classification = np.argmax(score)
    if classification == 0:
      true_positive+=1
    predictions_list_positives.append(classification)

  print("true positives : ",true_positive)
  print("false negatives : ",len(positives) - true_positive)

  # ----------------------------------------------------------------------------
  ## Run Over all negatives
  #-----------------------------------------------------------------------------

  print("\nAnalyzing negatives...\n")

  predictions_list_negatives = []
  ground_truth_list_negatives = np.ones(len(negatives))


  for sample in negatives:
    # read the image
    img = keras.preprocessing.image.load_img(str(sample), target_size=(256, 256))
    # trasform into array
    img_array = keras.preprocessing.image.img_to_array(img)
    img_array = tf.expand_dims(img_array, 0)
    
    # apply your inference here
    predictions = model.predict(img_array)
    score = tf.nn.softmax(predictions[0])
    
    # class_predict = tf.nn.sigmoid_cross_entropy_with_logits(labels=1, logits=np.max(predictions[0]))
    # print(np.max(class_predict))
    
    classification = np.argmax(score)
    if classification == 1:
      true_negative+=1
    predictions_list_negatives.append(classification)

  print("")
  print("true negatives : ",true_negative)
  print("false positives : ",len(negatives) - true_negative)
  # print(predictions_list_negatives)

  # ------------------------------------------------
  ## Creating lists for confusion matrix
  predictions = []
  ground_truth = []

  predictions = predictions_list_positives+predictions_list_negatives
  ground_truth = np.concatenate((ground_truth_list_positives, ground_truth_list_negatives))

  # print("lenght of predictions : ",len(predictions))
  # print("lenght of labels : ",len(ground_truth))

  ## Generating confusion matrix

  # Performance

  print("\n------------------------------------\nPERFORMANCE\n------------------------------------\n")

  # true positive rate
  sensitivity = true_positive/len(positives)
  print("sensitivity : ", round(sensitivity,3))

  # true negative rate
  specificity = true_negative/len(negatives)
  print("specificity : ",round(specificity,3))

  # accuracy
  accuracy = (true_positive + true_negative)/(len(positives) + len(negatives))
  print("accuracy :",round(accuracy,3))

  # F1-score
  f1_score = (2*true_positive)/(2*true_positive + false_positive + false_negative)
  print("f1_score : ",round(f1_score,3))
  print("\n------------------------------------\n")



  # GETTING THE CONFUSION MATRIX
  
  data = {'y_Actual': ground_truth,
        'y_Predicted': predictions
        }

  df = pd.DataFrame(data, columns=['y_Actual','y_Predicted'])
  confusion_matrix = pd.crosstab(df['y_Predicted'], df['y_Actual'], rownames=['Real values'], colnames=['Prediction'])

  sn.heatmap(confusion_matrix, cmap=plt.cm.Blues, annot=True, cbar=False, fmt='g', xticklabels= ['COVID','NON'], yticklabels= ['COVID','NON'])
  plt.title("PERFORMANCE OF : "+str(path_to_model))
  plt.show()

  # save as image
  # plt.savefig('./confusion_matrix_'+str(now)+'.png') #dpi = 200

"""**Specify here all the paths needed**"""

# MODEL PATH
path_to_model = '/content/drive/My Drive/vision/dataset/model.h5'

# POSITIVES PATH
path_to_dataset_covid = "/content/drive/My Drive/vision/test/covid"

# NEGATIVES PATH
path_to_dataset_nocovid = "/content/drive/My Drive/vision/test/non"

# dataset sources
# https://docs.google.com/spreadsheets/d/1icUEUYNJmEJlUUGf8EF750RuG5smZdefmwTsnTrIBzA/edit?usp=sharing

"""**Test the model over test_set**"""

inference(path_to_model,path_to_dataset_covid,path_to_dataset_nocovid)
